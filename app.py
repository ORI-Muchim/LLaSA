import os
import torch
import gradio as gr
import soundfile as sf
import numpy as np
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    AutoModelForSpeechSeq2Seq, 
    AutoProcessor, 
    pipeline
)
from xcodec2.modeling_xcodec2 import XCodec2Model
from FastAudioSR import FASR
from scipy import signal

import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

# Gradio ìºì‹± ì´ìŠˆ ë°©ì§€ë¥¼ ìœ„í•œ í™˜ê²½ ì„¤ì •
os.environ["GRADIO_ANALYTICS_ENABLED"] = "False"
os.environ["GRADIO_TEMP_DIR"] = "./temp_gradio"

# CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
print(f"ì‚¬ìš© ì¥ì¹˜: {device}, í…ì„œ íƒ€ì…: {torch_dtype}")

# ì„ì‹œ íŒŒì¼ì„ ìœ„í•œ ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs("temp", exist_ok=True)
os.makedirs("./temp_gradio", exist_ok=True)

# TTSì— í•„ìš”í•œ ìƒ˜í”Œ ë ˆì´íŠ¸ ì •ì˜
TARGET_SAMPLE_RATE = 16000  # 16kHz

# Whisper V3 ëª¨ë¸ ë¡œë“œ ì‹œë„
try:
    print("Whisper Large V3 Turbo ëª¨ë¸ ë¡œë“œ ì¤‘...")
    whisper_model_id = "openai/whisper-large-v3-turbo"
    
    whisper_model = AutoModelForSpeechSeq2Seq.from_pretrained(
        whisper_model_id, 
        torch_dtype=torch_dtype, 
        low_cpu_mem_usage=True, 
        use_safetensors=True
    )
    whisper_model.to(device)
    
    whisper_processor = AutoProcessor.from_pretrained(whisper_model_id)
    
    asr_pipe = pipeline(
        "automatic-speech-recognition",
        model=whisper_model,
        tokenizer=whisper_processor.tokenizer,
        feature_extractor=whisper_processor.feature_extractor,
        torch_dtype=torch_dtype,
        device=device,
    )
    
    print("Whisper Large V3 Turbo ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
    WHISPER_AVAILABLE = True
except Exception as e:
    print(f"Whisper ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
    print("ì „ì‚¬ ê¸°ëŠ¥ì€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
    WHISPER_AVAILABLE = False
    asr_pipe = None

# ì–¸ì–´ ëª¨ë¸ ë¡œë“œ
print("ì–¸ì–´ ëª¨ë¸ ë¡œë“œ ì¤‘...")
llasa_1b = "HKUSTAudio/Llasa-1B-multi-speakers-genshin-zh-en-ja-ko"
tokenizer = AutoTokenizer.from_pretrained(llasa_1b)
model = AutoModelForCausalLM.from_pretrained(llasa_1b)

# pad_token_id ëª…ì‹œì  ì„¤ì • (ê²½ê³  ë°©ì§€)
if tokenizer.pad_token_id is None:
    tokenizer.pad_token_id = tokenizer.eos_token_id

model.eval().to(device)
print("ì–¸ì–´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")

# ì˜¤ë””ì˜¤ ì½”ë± ëª¨ë¸ ë¡œë“œ
print("ì˜¤ë””ì˜¤ ì½”ë± ëª¨ë¸ ë¡œë“œ ì¤‘...")
codec_model_path = "HKUSTAudio/xcodec2"
codec_model = XCodec2Model.from_pretrained(codec_model_path)
codec_model.eval().to(device)
print("ì˜¤ë””ì˜¤ ì½”ë± ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")

# FastAudioSR ì—…ìƒ˜í”Œë§ ëª¨ë¸ ë¡œë“œ
print("FastAudioSR ì—…ìƒ˜í”Œë§ ëª¨ë¸ ë¡œë“œ ì¤‘...")
fasr = FASR("FastAudioSR/SR48K.pth")
print("FastAudioSR ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")

def ids_to_speech_tokens(speech_ids):
    """ìŒì„± IDë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜"""
    return [f"<|s_{speech_id}|>" for speech_id in speech_ids]

def extract_speech_ids(speech_tokens_str):
    """í† í°ì—ì„œ ìŒì„± ID ì¶”ì¶œ"""
    speech_ids = []
    for token_str in speech_tokens_str:
        if token_str.startswith('<|s_') and token_str.endswith('|>'):
            try:
                num = int(token_str[4:-2])
                speech_ids.append(num)
            except ValueError:
                print(f"ìœ íš¨í•˜ì§€ ì•Šì€ í† í° í˜•ì‹: {token_str}")
        else:
            print(f"ì˜ˆìƒì¹˜ ëª»í•œ í† í°: {token_str}")
    return speech_ids

def resample_audio(audio, orig_sr, target_sr):
    """ì˜¤ë””ì˜¤ë¥¼ target_srë¡œ ë¦¬ìƒ˜í”Œë§"""
    if orig_sr == target_sr:
        return audio
    
    # ë¦¬ìƒ˜í”Œë§ ê³„ìˆ˜ ê³„ì‚° ë° ë¦¬ìƒ˜í”Œë§ ìˆ˜í–‰
    resampled_audio = signal.resample_poly(audio, target_sr, orig_sr)
    return resampled_audio

def process_audio_file(audio_path, max_duration=10.0, target_sr=TARGET_SAMPLE_RATE):
    """ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬: ë¦¬ìƒ˜í”Œë§, ëª¨ë…¸ ë³€í™˜, ê¸¸ì´ ì¡°ì •"""
    # ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ
    audio, sr = sf.read(audio_path)
    
    # ìŠ¤í…Œë ˆì˜¤ë¥¼ ëª¨ë…¸ë¡œ ë³€í™˜
    if len(audio.shape) > 1:
        audio = audio[:, 0]
    
    # ìƒ˜í”Œ ë ˆì´íŠ¸ í™•ì¸ ë° ë¦¬ìƒ˜í”Œë§
    was_resampled = False
    if sr != target_sr:
        audio = resample_audio(audio, sr, target_sr)
        was_resampled = True
        print(f"ì˜¤ë””ì˜¤ë¥¼ {sr}Hzì—ì„œ {target_sr}Hzë¡œ ë¦¬ìƒ˜í”Œë§í–ˆìŠµë‹ˆë‹¤.")
    
    # ìµœëŒ€ ê¸¸ì´ í™•ì¸ ë° ì˜ë¼ë‚´ê¸°
    max_samples = int(max_duration * target_sr)
    was_trimmed = False
    if len(audio) > max_samples:
        audio = audio[:max_samples]
        was_trimmed = True
        print(f"ì˜¤ë””ì˜¤ë¥¼ {max_duration}ì´ˆë¡œ ì˜ëìŠµë‹ˆë‹¤.")
    
    # ì²˜ë¦¬ëœ ì˜¤ë””ì˜¤ ì €ì¥
    processed_path = os.path.join("temp", "processed_" + os.path.basename(audio_path))
    sf.write(processed_path, audio, target_sr)
    
    return processed_path, was_trimmed, was_resampled

def transcribe_audio(audio_path):
    """Whisperë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ ì „ì‚¬"""
    if asr_pipe is None:
        raise ValueError("Whisper ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    try:
        # Whisper V3 íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•œ ì „ì‚¬
        result = asr_pipe(audio_path)
        return result["text"]
    except Exception as e:
        print(f"ì „ì‚¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        raise

def generate_zero_shot_tts(reference_wav_path, reference_text, target_text):
    """ì œë¡œìƒ· TTS ìƒì„±"""
    # ì°¸ì¡° ì˜¤ë””ì˜¤ ì²˜ë¦¬ (ë¦¬ìƒ˜í”Œë§, ëª¨ë…¸ ë³€í™˜, ê¸¸ì´ ì¡°ì •)
    processed_path, was_trimmed, was_resampled = process_audio_file(reference_wav_path)
    
    # ì°¸ì¡° ì˜¤ë””ì˜¤ ë¡œë“œ
    ref_wav, sr = sf.read(processed_path)
    ref_wav = torch.from_numpy(ref_wav).float().unsqueeze(0).to(device)
    
    full_text = f"{reference_text} {target_text}"
    
    with torch.no_grad():
        # ì°¸ì¡° ì˜¤ë””ì˜¤ì—ì„œ ìŒì„± íŠ¹ì„± ì¸ì½”ë”©
        vq_code_prompt = codec_model.encode_code(input_waveform=ref_wav)
        vq_code_prompt = vq_code_prompt[0, 0, :]
        speech_tokens_prefix = ids_to_speech_tokens(vq_code_prompt)
        
        formatted_text = f"<|TEXT_UNDERSTANDING_START|>{full_text}<|TEXT_UNDERSTANDING_END|>"
        
        chat = [
            {"role": "user", "content": "Convert the text to speech:" + formatted_text},
            {"role": "assistant", "content": "<|SPEECH_GENERATION_START|>" + ''.join(speech_tokens_prefix)}
        ]
        
        # ì±„íŒ… í…œí”Œë¦¿ìœ¼ë¡œ ì…ë ¥ ì¤€ë¹„
        input_ids = tokenizer.apply_chat_template(
            chat, 
            tokenize=True, 
            return_tensors='pt', 
            continue_final_message=True
        ).to(device)
        
        # ê²½ê³  ë°©ì§€ë¥¼ ìœ„í•œ ì–´í…ì…˜ ë§ˆìŠ¤í¬ ìƒì„±
        attention_mask = torch.ones_like(input_ids)
        
        speech_end_id = tokenizer.convert_tokens_to_ids('<|SPEECH_GENERATION_END|>')
        
        # ìŒì„± í† í° ìƒì„±
        outputs = model.generate(
            input_ids,
            attention_mask=attention_mask,
            max_length=2048,
            eos_token_id=speech_end_id,
            pad_token_id=tokenizer.pad_token_id,
            do_sample=True,
            top_p=1,
            temperature=0.8,
        )
        
        # ìƒì„±ëœ í† í° ì²˜ë¦¬
        generated_ids = outputs[0][input_ids.shape[1]-len(speech_tokens_prefix):-1]
        speech_tokens = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
        speech_ids = extract_speech_ids(speech_tokens)
        speech_ids_tensor = torch.tensor(speech_ids).unsqueeze(0).unsqueeze(0).to(device)
        
        # ìŒì„± í† í°ì„ ì˜¤ë””ì˜¤ë¡œ ë””ì½”ë”©
        gen_wav = codec_model.decode_code(speech_ids_tensor)
        gen_wav = gen_wav[:, :, ref_wav.shape[1]:]
        
        # ìƒì„±ëœ ì˜¤ë””ì˜¤ ì €ì¥
        output_path = os.path.join("temp", "generated.wav")
        sf.write(output_path, gen_wav[0, 0, :].cpu().numpy(), TARGET_SAMPLE_RATE)
        
        # 16kHzì—ì„œ 48kHzë¡œ ì—…ìƒ˜í”Œë§
        upsampled_path = os.path.join("temp", "generated_48k.wav")
        fasr.run(output_path, upsampled_path)
        
        return upsampled_path, was_trimmed, was_resampled

def process_audio(reference_audio, auto_transcribe):
    """ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ ë° ì „ì‚¬ (ì„ íƒ ì‚¬í•­)"""
    if reference_audio is None:
        return None, "ì°¸ì¡° ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.", ""
    
    # ì˜¤ë””ì˜¤ ì²˜ë¦¬ (ë¦¬ìƒ˜í”Œë§, ëª¨ë…¸ ë³€í™˜, ê¸¸ì´ ì¡°ì •)
    processed_path, was_trimmed, was_resampled = process_audio_file(reference_audio)
    
    # ìƒíƒœ ë©”ì‹œì§€ ìƒì„±
    status_messages = []
    if was_resampled:
        status_messages.append(f"ì˜¤ë””ì˜¤ë¥¼ 16kHzë¡œ ë¦¬ìƒ˜í”Œë§í–ˆìŠµë‹ˆë‹¤.")
    if was_trimmed:
        status_messages.append(f"ì˜¤ë””ì˜¤ë¥¼ 10ì´ˆë¡œ ì˜ëìŠµë‹ˆë‹¤.")
    
    if not status_messages:
        status_messages.append("ì˜¤ë””ì˜¤ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    status = " ".join(status_messages)
    
    # ì „ì‚¬ ìš”ì²­ ì‹œ ì²˜ë¦¬
    transcription = ""
    if auto_transcribe:
        if not WHISPER_AVAILABLE:
            return processed_path, "Whisper ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì „ì‚¬ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.", ""
        try:
            transcription = transcribe_audio(processed_path)
            status += " ì „ì‚¬ ì™„ë£Œ."
            return processed_path, status, transcription
        except Exception as e:
            return processed_path, f"{status} ì „ì‚¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", ""
    else:
        return processed_path, status, ""

def generate_speech(reference_audio, reference_text, target_text):
    """TTS íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    try:
        # ì…ë ¥ ìœ íš¨ì„± ê²€ì‚¬
        if reference_audio is None:
            return None, "ì°¸ì¡° ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”."
        
        if not reference_text.strip():
            return None, "ì°¸ì¡° í…ìŠ¤íŠ¸ëŠ” ë¹„ì›Œë‘˜ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        if not target_text.strip():
            return None, "ìƒì„±í•  í…ìŠ¤íŠ¸ëŠ” ë¹„ì›Œë‘˜ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ìŒì„± ìƒì„± ì²˜ë¦¬
        output_path, was_trimmed, was_resampled = generate_zero_shot_tts(reference_audio, reference_text, target_text)
        
        # ê²°ê³¼ ë°˜í™˜
        message_parts = ["ìƒì„± ì™„ë£Œ!"]
        if was_resampled:
            message_parts.append("ì˜¤ë””ì˜¤ê°€ 16kHzë¡œ ë¦¬ìƒ˜í”Œë§ë˜ì—ˆìŠµë‹ˆë‹¤.")
        if was_trimmed:
            message_parts.append("ì°¸ì¡° ì˜¤ë””ì˜¤ê°€ 10ì´ˆë¡œ ì˜ë ¸ìŠµë‹ˆë‹¤.")
            
        message = " ".join(message_parts)
        
        return output_path, message
    except Exception as e:
        import traceback
        traceback.print_exc()
        return None, f"ì˜¤ë¥˜: {str(e)}"

# Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±
with gr.Blocks(theme=gr.themes.Soft(primary_hue="indigo")) as demo:
    gr.Markdown("""
    # ğŸ¤ ì œë¡œìƒ· í…ìŠ¤íŠ¸-ìŒì„± ë³€í™˜ ìƒì„±ê¸°
    
    ì°¸ì¡° ì˜¤ë””ì˜¤ ìƒ˜í”Œì„ ì—…ë¡œë“œí•˜ê³ , í•´ë‹¹ ì˜¤ë””ì˜¤ì—ì„œ ë§í•œ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•œ ë‹¤ìŒ, ê°™ì€ ëª©ì†Œë¦¬ë¡œ ìƒì„±í•˜ê³  ì‹¶ì€ ìƒˆ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.
    """)
    
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### ğŸ“ ì°¸ì¡° ì˜¤ë””ì˜¤ ë° í…ìŠ¤íŠ¸")
            reference_audio = gr.Audio(label="ì°¸ì¡° ì˜¤ë””ì˜¤ ì—…ë¡œë“œ (ìµœëŒ€ 10ì´ˆ)", type="filepath")
            
            auto_transcribe = gr.Checkbox(
                label="ìë™ ì „ì‚¬ (Whisper Large V3 Turbo ì‚¬ìš©)", 
                value=False,
                interactive=WHISPER_AVAILABLE
            )
            
            if not WHISPER_AVAILABLE:
                gr.Markdown("âš ï¸ ì „ì‚¬ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤: Whisper ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            process_btn = gr.Button("ì˜¤ë””ì˜¤ ì²˜ë¦¬", variant="primary")
            audio_status = gr.Textbox(label="ìƒíƒœ", interactive=False)
            
            reference_text = gr.Textbox(
                label="ì°¸ì¡° í…ìŠ¤íŠ¸ (ì˜¤ë””ì˜¤ì— ë‹´ê¸´ ë‚´ìš©)", 
                lines=3, 
                placeholder="ì°¸ì¡° ì˜¤ë””ì˜¤ì—ì„œ ë§í•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."
            )
            
        with gr.Column(scale=1):
            gr.Markdown("### ğŸ”Š ìŒì„± ìƒì„±")
            target_text = gr.Textbox(
                label="ìƒì„±í•  í…ìŠ¤íŠ¸", 
                lines=5, 
                placeholder="ë™ì¼í•œ ëª©ì†Œë¦¬ë¡œ ìƒì„±í•˜ê³  ì‹¶ì€ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."
            )
            generate_btn = gr.Button("ìŒì„± ìƒì„±", variant="primary")
            generation_status = gr.Textbox(label="ìƒíƒœ", interactive=False)
            output_audio = gr.Audio(label="ìƒì„±ëœ ì˜¤ë””ì˜¤")
            
            # ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ë¥¼ output_audio ì•„ë˜ë¡œ ì´ë™
            gr.Markdown("""
            ### â„¹ï¸ ì°¸ê³ ì‚¬í•­
            - ëª¨ë“  ì˜¤ë””ì˜¤ëŠ” ìë™ìœ¼ë¡œ 16kHzë¡œ ë¦¬ìƒ˜í”Œë§ë©ë‹ˆë‹¤
            - ì°¸ì¡° ì˜¤ë””ì˜¤ê°€ 10ì´ˆë³´ë‹¤ ê¸¸ë©´ ìë™ìœ¼ë¡œ ì˜ë¦½ë‹ˆë‹¤
            - ì°¸ì¡° ì˜¤ë””ì˜¤ì˜ í’ˆì§ˆì´ ìŒì„± í´ë¡œë‹ í’ˆì§ˆì— ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤
            - ìµœìƒì˜ ê²°ê³¼ë¥¼ ìœ„í•´ ë°°ê²½ ì¡ìŒì´ ì ì€ ê¹¨ë—í•œ ì˜¤ë””ì˜¤ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”
            - Whisper Large V3 Turbo ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìë™ ì „ì‚¬ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤
            """)
    
    # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì„¤ì •
    process_btn.click(
        fn=process_audio,
        inputs=[reference_audio, auto_transcribe],
        outputs=[reference_audio, audio_status, reference_text]
    )
    
    generate_btn.click(
        fn=generate_speech,
        inputs=[reference_audio, reference_text, target_text],
        outputs=[output_audio, generation_status]
    )

# ì•± ì‹¤í–‰
if __name__ == "__main__":
    try:
        # ê³µìœ  ë§í¬ ìƒì„±ì„ ìœ„í•´ share=True ê°•ì œ ì ìš©
        print("ê³µìœ  ë§í¬ ìƒì„±ì„ ìœ„í•´ share=Trueë¡œ Gradio ì‹¤í–‰ ì¤‘...")
        demo.launch(share=True, server_name="0.0.0.0")
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"Gradio ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        # ì²« ë²ˆì§¸ ë°©ë²•ì´ ì‹¤íŒ¨í•˜ë©´ ëŒ€ì²´ ì‹¤í–‰ ë°©ë²• ì‹œë„
        try:
            print("ëŒ€ì²´ ì‹¤í–‰ ë°©ë²• ì‹œë„ ì¤‘...")
            demo.launch(share=True)
        except Exception as e2:
            print(f"ëŒ€ì²´ ì‹¤í–‰ ë°©ë²•ë„ ì‹¤íŒ¨: {e2}")
